{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard functions for dataloading and meta-classifier training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import numpy as np\n",
    "import graph_tool as gt\n",
    "import ast\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x33a3bf7b0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_results_df(data_dir):\n",
    "  # read results into dataframe\n",
    "  NSHARDS = 10\n",
    "\n",
    "  dfs = []\n",
    "  for shard_idx in range(NSHARDS):\n",
    "    filename = 'results.ndjson-%s-of-%s' % (str(shard_idx).zfill(5), str(NSHARDS).zfill(5))\n",
    "    print(filename)\n",
    "\n",
    "    with open(f'{data_dir}/{filename}', 'r') as f:\n",
    "      lines = f.readlines()\n",
    "      records = map(json.loads, lines)\n",
    "      dfs.append(pd.DataFrame.from_records(records))\n",
    "\n",
    "  # Construct df and remove nans\n",
    "  results_df = pd.concat(dfs)\n",
    "  results_df.drop(['marginal_param', 'fixed_params'], axis=1, inplace=True)\n",
    "  results_df.dropna(axis=0, inplace=True)\n",
    "  del dfs\n",
    "  return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data(graph_id, data_dir, results_df):\n",
    "    \n",
    "    gt_graph = gt.load_graph(data_dir + '{}_graph.gt'.format(graph_id))\n",
    "    # Extract edges\n",
    "    src, dst = gt_graph.get_edges().T\n",
    "    src, dst = torch.tensor(src, dtype = torch.int64), torch.tensor(dst, dtype = torch.int64)\n",
    "\n",
    "    # Load node features\n",
    "    node_feats = torch.tensor(np.loadtxt(data_dir+'{}_node_features.txt'.format(graph_id)), dtype=torch.float32)\n",
    "\n",
    "    # Verify shape (should be [num_nodes, 16])\n",
    "    assert node_feats.dim() == 2 and node_feats.size(1) == 16\n",
    "    \n",
    "    # Instead of adding self-loops later, rebuild the edge list with self-loops.\n",
    "    num_nodes = node_feats.shape[0]\n",
    "    # Convert src and dst to lists (or numpy arrays) and add self-loops.\n",
    "    src_list = src.tolist() + list(range(num_nodes))\n",
    "    dst_list = dst.tolist() + list(range(num_nodes))\n",
    "    \n",
    "    # Build a new graph with the combined edge list.\n",
    "    g = dgl.graph((src_list, dst_list), num_nodes=num_nodes)\n",
    "\n",
    "    g.ndata['feat'] = node_feats\n",
    "\n",
    "    # Load labels\n",
    "    matched_df = results_df[results_df['sample_id'] == int(graph_id)]\n",
    "    assert matched_df.shape[0] == 1, \"Expected exactly one matching row in results_df, however found {}\".format(matched_df.shape[0])\n",
    "    label_mlp = matched_df[''].item()\n",
    "    label_gcn = matched_df['GCN__test_accuracy'].item()\n",
    "    probs = F.softmax(torch.tensor([label_mlp, label_gcn], dtype=torch.float32), dim=0)\n",
    "    g.label = probs \n",
    "\n",
    "    g.graph_metrics_results = matched_df.to_dict()\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGraphDataset(DGLDataset):\n",
    "    def __init__(self, graph_dir, num_graphs = None):\n",
    "        self.graph_dir = graph_dir\n",
    "        self.num_graphs = num_graphs\n",
    "        self.dim_nfeats = None\n",
    "        self.gclasses = None\n",
    "        self.results_df = extract_results_df(graph_dir)\n",
    "        super().__init__(name='my_graph_dataset')\n",
    "\n",
    "    def _gen_graphs_ids(self):\n",
    "\n",
    "        def cut_to_five(id:str):\n",
    "            if len(id) == 5:\n",
    "                return id\n",
    "            else:\n",
    "                id = id[1:]\n",
    "                return cut_to_five(id)\n",
    "    \n",
    "        IDS = ['0000' + str(i) for i in range(0,self.num_graphs)]\n",
    "        IDS = [cut_to_five(id) for id in IDS]\n",
    "\n",
    "        return IDS \n",
    "\n",
    "    def process(self):\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        graph_ids = self._gen_graphs_ids()\n",
    "\n",
    "        for gid in graph_ids:\n",
    "\n",
    "            g = load_graph_data(gid, self.graph_dir, self.results_df)\n",
    "\n",
    "            self.graphs.append(g)\n",
    "            self.labels.append(g.label)\n",
    "        \n",
    "        self.dim_nfeats = self.graphs[0].ndata['feat'].shape[1]\n",
    "        self.gclasses = self.graphs[0].label.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def has_cache(self): return False\n",
    "    def download(self): pass\n",
    "    def save(self): pass\n",
    "    def load(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_dataloader: GraphDataLoader, test_dataloader: GraphDataLoader, val_dataloader:GraphDataLoader, optimizer: torch.optim.Optimizer, epochs : int):\n",
    "    \n",
    "    wandb.init(project=\"meta-classifier dev\", reinit=True)\n",
    "    wandb.watch(model)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for batched_graph, labels in train_dataloader:            \n",
    "            pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "            log_pred = F.log_softmax(pred, dim=1)\n",
    "            loss = F.kl_div(log_pred, labels, reduction='batchmean')\n",
    "            wandb.log({\"train_loss\": loss})\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        num_correct = 0\n",
    "        num_tests = 0\n",
    "        for batched_graph, labels in val_dataloader:\n",
    "            pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "            num_correct += (pred.argmax(1) == labels.argmax(1)).sum().item()\n",
    "            num_tests += len(labels)\n",
    "        wandb.log({\"val_accuracy\": num_correct / num_tests})\n",
    "        print(\"Validation accuracy:\", num_correct / num_tests)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_tests = 0\n",
    "    for batched_graph, labels in test_dataloader:\n",
    "        pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "        num_correct += (pred.argmax(1) == labels.argmax(1)).sum().item()\n",
    "        num_tests += len(labels)\n",
    "    wandb.log({\"test_accuracy\": num_correct / num_tests})\n",
    "\n",
    "    print(\"Test accuracy:\", num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata[\"h\"] = h\n",
    "        return dgl.mean_nodes(g, \"h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GCN on small (100 graphs) locally generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "locally_gen_dir = \"../../graph_gen/locally_gen/nodeclassification/sbm/\"\n",
    "dataset = MyGraphDataset(locally_gen_dir, num_graphs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(dataset)\n",
    "num_train = int(num_examples * 0.9)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train), generator=g)\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples), generator=g)\n",
    "\n",
    "train_dataloader = GraphDataLoader(dataset, sampler=train_sampler, batch_size=5, drop_last=False, generator=g)\n",
    "test_dataloader = GraphDataLoader(dataset, sampler=test_sampler, batch_size=5, drop_last=False, generator=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross entropy loss i.e. hard labels\n",
      "Test accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Create the model with given dimensions\n",
    "model = GCN(dataset.dim_nfeats, 16, dataset.gclasses)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for batched_graph, labels in train_dataloader:\n",
    "        pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "        labels = labels.argmax(dim=1)\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "for batched_graph, labels in test_dataloader:\n",
    "    pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "    num_correct += (pred.argmax(1) == labels.argmax(1)).sum().item()\n",
    "    num_tests += len(labels)\n",
    "\n",
    "print(\"Using cross entropy loss i.e. hard labels\")\n",
    "print(\"Test accuracy:\", num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using KL_div loss i.e. soft labels\n",
      "Test accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Create the model with given dimensions\n",
    "model = GCN(dataset.dim_nfeats, 16, dataset.gclasses)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for batched_graph, labels in train_dataloader:\n",
    "        pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "        log_pred = F.log_softmax(pred, dim=1)\n",
    "        loss = F.kl_div(log_pred, labels, reduction='batchmean')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "for batched_graph, labels in test_dataloader:\n",
    "    pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "    num_correct += (pred.argmax(1) == labels.argmax(1)).sum().item()\n",
    "    num_tests += len(labels)\n",
    "\n",
    "print(\"Using KL_div loss i.e. soft labels\")\n",
    "print(\"Test accuracy:\", num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on first Snellius batch: 500 examples, feature dimension 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.ndjson-00000-of-00010\n",
      "results.ndjson-00001-of-00010\n",
      "results.ndjson-00002-of-00010\n",
      "results.ndjson-00003-of-00010\n",
      "results.ndjson-00004-of-00010\n",
      "results.ndjson-00005-of-00010\n",
      "results.ndjson-00006-of-00010\n",
      "results.ndjson-00007-of-00010\n",
      "results.ndjson-00008-of-00010\n",
      "results.ndjson-00009-of-00010\n"
     ]
    }
   ],
   "source": [
    "snellius_data_dir = \"../../graph_gen/snellius_gen/nodeclassification/sbm/\"\n",
    "snellius_dataset = MyGraphDataset(snellius_data_dir, num_graphs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(snellius_dataset)\n",
    "num_train = int(num_examples * 0.6)\n",
    "num_val = int(num_examples * 0.2)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train), generator=g)\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_train+num_val), generator=g)\n",
    "val_sampler = SubsetRandomSampler(torch.arange(num_train+num_val, num_examples), generator=g)\n",
    "\n",
    "train_dataloader = GraphDataLoader(snellius_dataset, sampler=train_sampler, batch_size=20, drop_last=False, generator=g)\n",
    "test_dataloader = GraphDataLoader(snellius_dataset, sampler=test_sampler, batch_size=20, drop_last=False, generator=g)\n",
    "val_dataloader = GraphDataLoader(snellius_dataset, sampler=val_sampler, batch_size=20, drop_last=False, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>▇█▅▃▄▄▃▂▃▂▂▂▂▁▂▁▂▂▁▂▁▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>val_accuracy</td><td>▁█▁▁▃▃▄▄▆▆▆▄▄▄▆▄▄▄▄▄▄▄▄▄▄▄▄▄█▆█▆█▆▄▄▄█▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.35</td></tr><tr><td>train_loss</td><td>0.0001</td></tr><tr><td>val_accuracy</td><td>0.45</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-brook-6</strong> at: <a href='https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev/runs/xlprejbq' target=\"_blank\">https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev/runs/xlprejbq</a><br> View project at: <a href='https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev' target=\"_blank\">https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_155322-xlprejbq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/daniel/Documents/masters_ai/thesis/graphworld/src/meta_classifier/wandb/run-20250407_155341-v44g5mqz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev/runs/v44g5mqz' target=\"_blank\">treasured-wind-7</a></strong> to <a href='https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev' target=\"_blank\">https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev/runs/v44g5mqz' target=\"_blank\">https://wandb.ai/daniel-goodwin-university-of-amsterdam/meta-classifier%20dev/runs/v44g5mqz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Validation accuracy: 0.37\n",
      "Epoch: 1\n",
      "Validation accuracy: 0.46\n",
      "Epoch: 2\n",
      "Validation accuracy: 0.41\n",
      "Epoch: 3\n",
      "Validation accuracy: 0.47\n",
      "Epoch: 4\n",
      "Validation accuracy: 0.52\n",
      "Epoch: 5\n",
      "Validation accuracy: 0.53\n",
      "Epoch: 6\n",
      "Validation accuracy: 0.57\n",
      "Epoch: 7\n",
      "Validation accuracy: 0.52\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.55\n",
      "Epoch: 9\n",
      "Validation accuracy: 0.58\n",
      "Epoch: 10\n",
      "Validation accuracy: 0.44\n",
      "Epoch: 11\n",
      "Validation accuracy: 0.5\n",
      "Epoch: 12\n",
      "Validation accuracy: 0.46\n",
      "Epoch: 13\n",
      "Validation accuracy: 0.47\n",
      "Epoch: 14\n",
      "Validation accuracy: 0.5\n",
      "Epoch: 15\n",
      "Validation accuracy: 0.42\n",
      "Epoch: 16\n",
      "Validation accuracy: 0.45\n",
      "Epoch: 17\n",
      "Validation accuracy: 0.46\n",
      "Epoch: 18\n",
      "Validation accuracy: 0.46\n",
      "Epoch: 19\n",
      "Validation accuracy: 0.48\n",
      "Epoch: 20\n",
      "Validation accuracy: 0.44\n",
      "Epoch: 21\n",
      "Validation accuracy: 0.45\n",
      "Epoch: 22\n",
      "Validation accuracy: 0.45\n",
      "Epoch: 23\n",
      "Validation accuracy: 0.48\n",
      "Epoch: 24\n",
      "Validation accuracy: 0.49\n",
      "Epoch: 25\n",
      "Validation accuracy: 0.52\n",
      "Epoch: 26\n",
      "Validation accuracy: 0.4\n",
      "Epoch: 27\n",
      "Validation accuracy: 0.48\n",
      "Epoch: 28\n",
      "Validation accuracy: 0.5\n",
      "Epoch: 29\n",
      "Validation accuracy: 0.46\n",
      "Epoch: 30\n",
      "Validation accuracy: 0.44\n",
      "Epoch: 31\n",
      "Validation accuracy: 0.48\n",
      "Epoch: 32\n",
      "Validation accuracy: 0.5\n",
      "Epoch: 33\n",
      "Validation accuracy: 0.58\n",
      "Epoch: 34\n",
      "Validation accuracy: 0.45\n",
      "Epoch: 35\n",
      "Validation accuracy: 0.43\n",
      "Epoch: 36\n",
      "Validation accuracy: 0.44\n",
      "Epoch: 37\n",
      "Validation accuracy: 0.48\n",
      "Epoch: 38\n",
      "Validation accuracy: 0.44\n",
      "Epoch: 39\n",
      "Validation accuracy: 0.47\n",
      "Epoch: 40\n",
      "Validation accuracy: 0.47\n",
      "Epoch: 41\n",
      "Validation accuracy: 0.43\n",
      "Epoch: 42\n",
      "Validation accuracy: 0.48\n",
      "Epoch: 43\n",
      "Validation accuracy: 0.5\n",
      "Epoch: 44\n",
      "Validation accuracy: 0.45\n",
      "Epoch: 45\n",
      "Validation accuracy: 0.48\n",
      "Epoch: 46\n",
      "Validation accuracy: 0.47\n",
      "Epoch: 47\n",
      "Validation accuracy: 0.47\n",
      "Epoch: 48\n",
      "Validation accuracy: 0.48\n",
      "Epoch: 49\n",
      "Validation accuracy: 0.5\n",
      "Test accuracy: 0.48\n"
     ]
    }
   ],
   "source": [
    "model = GCN(snellius_dataset.dim_nfeats, 16, snellius_dataset.gclasses)\n",
    "train(model, train_dataloader, test_dataloader, val_dataloader, torch.optim.Adam(model.parameters(), lr=0.01), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
